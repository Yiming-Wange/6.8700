{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "looking-belize",
      "metadata": {
        "id": "looking-belize"
      },
      "source": [
        "# 6.8701 PSET 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fatty-banana",
      "metadata": {
        "id": "fatty-banana"
      },
      "source": [
        "**Deadline: Tues Nov 12th 11:59pm**\n",
        "\n",
        "In this homework, you will get some practical experience with modeling small molecules, docking, and understanding eQTLs. The goal is to expose you to useful libraries (torch, rdkit, dgl) that are heavily utilized in practice, understand some of the state of the art research in structural biology, and practice with genetic association studies by studying eQTLs.\n",
        "\n",
        "The questions are designed in a way that you should not need to know these libraries to answer them, but if anything is confusing or if you get stuck please ask for help on Piazza!\n",
        "\n",
        "Answer all questions directly in this notebook and complete the missing code where marked with **COMPLETE HERE**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "experimental-mediterranean",
      "metadata": {
        "id": "experimental-mediterranean"
      },
      "source": [
        "## 0. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "inner-phrase",
      "metadata": {
        "id": "inner-phrase"
      },
      "source": [
        "Run the cell below to import all the relevant libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "blind-zimbabwe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blind-zimbabwe",
        "outputId": "17cae328-f693-4b19-de14-40393cd03164"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://data.dgl.ai/wheels/torch-2.3/repo.html\n",
            "Collecting dgl\n",
            "  Downloading https://data.dgl.ai/wheels/torch-2.3/dgl-2.4.0-cp310-cp310-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgl) (3.4.2)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from dgl) (24.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgl) (2.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (5.9.5)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.9.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from dgl) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgl) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgl) (4.66.6)\n",
            "Collecting torch<=2.4.0 (from dgl)\n",
            "  Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (2.23.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->dgl) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->dgl) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (3.16.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (1.13.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<=2.4.0->dgl) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch<=2.4.0->dgl)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.0.0 (from torch<=2.4.0->dgl)\n",
            "  Downloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch<=2.4.0->dgl) (12.6.77)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->dgl) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->dgl) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<=2.4.0->dgl) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<=2.4.0->dgl) (1.3.0)\n",
            "Downloading torch-2.4.0-cp310-cp310-manylinux1_x86_64.whl (797.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.2/797.2 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-3.0.0-1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nccl-cu12, nvidia-cusparse-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusolver-cu12, nvidia-cudnn-cu12, torch, dgl\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.4.2\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.4.2:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.4.2\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.7.77\n",
            "    Uninstalling nvidia-curand-cu12-10.3.7.77:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.7.77\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.3.0.4\n",
            "    Uninstalling nvidia-cufft-cu12-11.3.0.4:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.3.0.4\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.6.77\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.6.77:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.6.77\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.6.80\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.6.80:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.6.80\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.6.3.3\n",
            "    Uninstalling nvidia-cublas-cu12-12.6.3.3:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.6.3.3\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.7.1.2\n",
            "    Uninstalling nvidia-cusolver-cu12-11.7.1.2:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.7.1.2\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.5.1.17\n",
            "    Uninstalling nvidia-cudnn-cu12-9.5.1.17:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.5.1.17\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.5.0+cu121\n",
            "    Uninstalling torch-2.5.0+cu121:\n",
            "      Successfully uninstalled torch-2.5.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.4.0 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dgl-2.4.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvtx-cu12-12.1.105 torch-2.4.0 triton-3.0.0\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.8.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.7)\n",
            "Requirement already satisfied: numpy<2,>=1.21 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (24.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Collecting torch==2.3.1\n",
            "  Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (1.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.1)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch==2.3.1) (12.1.105)\n",
            "Collecting triton==2.3.1 (from torch==2.3.1)\n",
            "  Downloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.1) (12.6.77)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.3.1) (1.3.0)\n",
            "Downloading torch-2.3.1-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triton-2.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: triton, nvidia-cudnn-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 3.0.0\n",
            "    Uninstalling triton-3.0.0:\n",
            "      Successfully uninstalled triton-3.0.0\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.1.0.70\n",
            "    Uninstalling nvidia-cudnn-cu12-9.1.0.70:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.1.0.70\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.4.0\n",
            "    Uninstalling torch-2.4.0:\n",
            "      Successfully uninstalled torch-2.4.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.5.0+cu121 requires torch==2.5.0, but you have torch 2.3.1 which is incompatible.\n",
            "torchvision 0.20.0+cu121 requires torch==2.5.0, but you have torch 2.3.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-8.9.2.26 torch-2.3.1 triton-2.3.1\n",
            "Collecting rdkit\n",
            "  Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from rdkit) (1.26.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from rdkit) (10.4.0)\n",
            "Downloading rdkit-2024.3.5-cp310-cp310-manylinux_2_28_x86_64.whl (33.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.1/33.1 MB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdkit\n",
            "Successfully installed rdkit-2024.3.5\n",
            "Collecting dgllife\n",
            "  Downloading dgllife-0.3.2-py3-none-any.whl.metadata (667 bytes)\n",
            "Requirement already satisfied: scikit-learn>=0.22.2 in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from dgllife) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.10/dist-packages (from dgllife) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from dgllife) (4.66.6)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.13.1)\n",
            "Requirement already satisfied: networkx>=2.1 in /usr/local/lib/python3.10/dist-packages (from dgllife) (3.4.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.10/dist-packages (from dgllife) (0.2.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from dgllife) (1.4.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.22.0->dgllife) (2024.8.30)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22.2->dgllife) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (1.16.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (3.1.0)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.10/dist-packages (from hyperopt->dgllife) (0.10.9.7)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->dgllife) (2024.2)\n",
            "Downloading dgllife-0.3.2-py3-none-any.whl (226 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.1/226.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: dgllife\n",
            "Successfully installed dgllife-0.3.2\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.6)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.13.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Collecting scanpy\n",
            "  Downloading scanpy-1.10.3-py3-none-any.whl.metadata (9.4 kB)\n",
            "Collecting anndata>=0.8 (from scanpy)\n",
            "  Downloading anndata-0.10.9-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: h5py>=3.1 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.12.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.4.2)\n",
            "Collecting legacy-api-wrap>=1.4 (from scanpy)\n",
            "  Downloading legacy_api_wrap-1.4-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.8.0)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: networkx>=2.7 in /usr/local/lib/python3.10/dist-packages (from scanpy) (3.4.2)\n",
            "Requirement already satisfied: numba>=0.56 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.26.4)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scanpy) (24.1)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.10/dist-packages (from scanpy) (2.2.2)\n",
            "Requirement already satisfied: patsy in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.5.6)\n",
            "Collecting pynndescent>=0.5 (from scanpy)\n",
            "  Downloading pynndescent-0.5.13-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: scikit-learn>=0.24 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from scanpy) (1.13.1)\n",
            "Requirement already satisfied: seaborn>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.13.2)\n",
            "Collecting session-info (from scanpy)\n",
            "  Downloading session_info-1.0.0.tar.gz (24 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: statsmodels>=0.13 in /usr/local/lib/python3.10/dist-packages (from scanpy) (0.14.4)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from scanpy) (4.66.6)\n",
            "Collecting umap-learn!=0.5.0,>=0.5 (from scanpy)\n",
            "  Downloading umap_learn-0.5.7-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting array-api-compat!=1.5,>1.4 (from anndata>=0.8->scanpy)\n",
            "  Downloading array_api_compat-1.9.1-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anndata>=0.8->scanpy) (1.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (4.54.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (1.4.7)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (10.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (3.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.6->scanpy) (2.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.56->scanpy) (0.43.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.5->scanpy) (2024.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.24->scanpy) (3.5.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy->scanpy) (1.16.0)\n",
            "Collecting stdlib_list (from session-info->scanpy)\n",
            "  Downloading stdlib_list-0.11.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Downloading scanpy-1.10.3-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading anndata-0.10.9-py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.0/129.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading legacy_api_wrap-1.4-py3-none-any.whl (15 kB)\n",
            "Downloading pynndescent-0.5.13-py3-none-any.whl (56 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading umap_learn-0.5.7-py3-none-any.whl (88 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.8/88.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading array_api_compat-1.9.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stdlib_list-0.11.0-py3-none-any.whl (83 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.6/83.6 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: session-info\n",
            "  Building wheel for session-info (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for session-info: filename=session_info-1.0.0-py3-none-any.whl size=8023 sha256=228b1b5a9edcfdca2f671f0d2942513dfa083e69f3e3b01e480371688a3a499b\n",
            "  Stored in directory: /root/.cache/pip/wheels/6a/aa/b9/eb5d4031476ec10802795b97ccf937b9bd998d68a9b268765a\n",
            "Successfully built session-info\n",
            "Installing collected packages: stdlib_list, legacy-api-wrap, array-api-compat, session-info, pynndescent, anndata, umap-learn, scanpy\n",
            "Successfully installed anndata-0.10.9 array-api-compat-1.9.1 legacy-api-wrap-1.4 pynndescent-0.5.13 scanpy-1.10.3 session-info-1.0.0 stdlib_list-0.11.0 umap-learn-0.5.7\n",
            "Collecting leidenalg\n",
            "  Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting igraph<0.12,>=0.10.0 (from leidenalg)\n",
            "  Downloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting texttable>=1.6.2 (from igraph<0.12,>=0.10.0->leidenalg)\n",
            "  Downloading texttable-1.7.0-py2.py3-none-any.whl.metadata (9.8 kB)\n",
            "Downloading leidenalg-0.10.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading igraph-0.11.8-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading texttable-1.7.0-py2.py3-none-any.whl (10 kB)\n",
            "Installing collected packages: texttable, igraph, leidenalg\n",
            "Successfully installed igraph-0.11.8 leidenalg-0.10.2 texttable-1.7.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n"
          ]
        }
      ],
      "source": [
        "!pip install  dgl -f https://data.dgl.ai/wheels/torch-2.3/repo.html # errors happen with torch==2.5.0\n",
        "!pip install matplotlib\n",
        "!pip install scikit-learn\n",
        "!pip install torch==2.3.1 # errors happen with torch==2.5.0\n",
        "!pip install rdkit\n",
        "!pip install dgllife\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install scipy\n",
        "!pip install numpy\n",
        "!pip install scanpy\n",
        "!pip install leidenalg\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.datasets import make_gaussian_quantiles\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "from dgllife.data import Tox21\n",
        "from dgllife.utils import SMILESToBigraph, CanonicalAtomFeaturizer, CanonicalBondFeaturizer\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import AllChem\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import dgl\n",
        "\n",
        "import scanpy as sc\n",
        "import anndata\n",
        "import pandas as pd\n",
        "import scipy.io as io\n",
        "from google.colab import files\n",
        "from scipy.spatial.transform import Rotation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrapped-image",
      "metadata": {
        "id": "wrapped-image"
      },
      "source": [
        "## 1. Introduction to Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YhP166Ty9XpC",
      "metadata": {
        "id": "YhP166Ty9XpC"
      },
      "source": [
        "In the previous problem set (PSET 2), we implemented a simple logistic regression model and an active learning-based model. In this PSET, we will be working with more complex models such as RNNs (LSTMs) and GNNs, where a strong technical understanding is necessary. This section will go more in-depth on the fundamentals."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "julian-stream",
      "metadata": {
        "id": "julian-stream"
      },
      "source": [
        "### 1.A Single Layer Network & Logistic Regression\n",
        "\n",
        "The central paradigm of machine learning is that there is some true function $f(\\vec{x}) = \\vec{y}$ generating our data. Our objective is to learn a model with tunable parameters that can approximate this true function. Ideally, this model could make new predictions and reveal novel insights on the data. For small molecules, $\\vec{x}$ is a representation of such a molecule, and $\\vec{y}$ are biochemical properties such as solubility, binding, and efficacy in a biological context.\n",
        "\n",
        "One of the most fundamental and basic model used for this purpose is a Multi-Layer Perceptron (MLP), also known as a feed-forward network (FFN). These networks consist of \"fully connected layers\" stacked on top of each other with activation functions between the layers.\n",
        "\n",
        "A single linear layer has operations boil down to basic linear algebra:\n",
        "\n",
        "$$\\vec{y} = \\vec{x} \\textbf{W} + \\vec{b}$$\n",
        "\n",
        "Here, $\\vec{x}$ is a feature vector size $n$, which represents the input to the model:\n",
        "\n",
        "$$ x = [x_0, x_1, ... x_n] $$\n",
        "\n",
        "Our goal is to learn the weight matrix $\\textbf{W}$ and the bias $\\vec{b}$ that transform $\\vec{x}$ into $\\vec{y}$. When linear layers are stacked on top of each other with activation functions in between, they form what are known as MLPs or FFNs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "chubby-agreement",
      "metadata": {
        "id": "chubby-agreement"
      },
      "source": [
        "#### 1.A.1 Question (1pt)\n",
        "\n",
        "Assume that the input data $\\vec{x}$ is 10-dimensional (i.e there are 10 features per sample), and the output $\\vec{y}$ is 3-dimensional, what is the total number of parameters learned in the formulation presented above? Assume it is a single linear layer."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "republican-society",
      "metadata": {
        "id": "republican-society"
      },
      "source": [
        "**ANSWER**:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lucky-intro",
      "metadata": {
        "id": "lucky-intro"
      },
      "source": [
        "#### 1.A.2 Question (2pt)\n",
        "\n",
        "So, suppose that we have $f(\\vec{x}) = \\vec{y}$. The idea is that $f(\\vec{x})$ is unknown, but we have many datapoints of $(x_i, y_i)$. Can we find some $\\textbf{W}, \\textbf{b}$ such that $$\\vec{y_i} \\approx \\vec{x_i} \\textbf{W} + \\vec{b}$$ for all the datapoints? If we can learn those parameters, we could make predicitons on unseen data.\n",
        "\n",
        "Finding the best parameters, a process known as training, typically involves optimizing an objective function. Intuitively, this means adjusting the weights and biases so that the loss function, the measure of how far our model's predictions deviate from the actual data, is minimized. In principle, the lower the loss or objective function is, the better the model describes the data. Ideally, the model not only fits the training data well but also generalizes to new, unseen data.\n",
        "\n",
        "We will use the cross entropy loss:\n",
        "\n",
        "$$Loss(y_{pred}, y_{true}) = - (y_{true} \\cdot log(y_{pred}) + (1 - y_{true}) \\cdot log(1 - y_{pred})) $$\n",
        "\n",
        "Here *y_true* is 1 for a positive example and 0 for negatives, and *y_pred* is a probability for the positive class ranging from 0 to 1. For simpliciy, we will assume that our input $\\vec{x}$ is 2-dimensional. In order to obtain a probability (between 0 and 1) we must normalize the output $y_{pred}$. We do so using the sigmoid function:\n",
        "\n",
        "$$ y^{pred} = sigmoid(\\vec{x}W + \\vec{b}) = \\frac{e^{\\vec{x}W + \\vec{b}}}{1 + e^{\\vec{x}W + \\vec{b}}}$$\n",
        "\n",
        "In order to learn *W* and *b*, we can use gradient descent: we comptue the derivative of the loss with respect to the parameters.\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    \\frac{dLoss}{dW} &= (y_{pred} - y_{true})x \\\\\n",
        "    \\frac{dLoss}{db} &= (y_{pred} - y_{true})\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "The loss function tells us how bad the model parameters are, and we want to make the model less bad. The gradient gives us an estimate of the correct direction to change the parameters to make the model less bad. After enough iterations, ideally the model is good enough to explain not just training data but also held out data.\n",
        "\n",
        "Implement the functions below to perform logistic regression on two synthetic datasets. This is important practice before moving to actual small molecules. It may be useful to print the shape of the input to make sure you are handling dimensions correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reasonable-enclosure",
      "metadata": {
        "id": "reasonable-enclosure"
      },
      "outputs": [],
      "source": [
        "# Generate synthetic datasets for binary classification\n",
        "# Dataset 1: Linearly separable data\n",
        "# Dataset 2: Data distributed across Gaussian quantiles\n",
        "# Both datasets consist of 1000 samples with 2 features each\n",
        "X1, Y1 = make_classification(n_samples=1000, n_features=2, n_redundant=0, n_repeated=0, class_sep=5.0, n_classes=2, random_state=1)\n",
        "Y1 = Y1[..., None]\n",
        "X2, Y2 = make_gaussian_quantiles(n_samples=1000, n_features=2, n_classes=2, random_state=1)\n",
        "Y2 = Y2[..., None]\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation of the input x.\"\"\"\n",
        "    # COMPLETE HERE (hint: use np.exp)\n",
        "    # Output should have the same shape as the input (num_samples, 1)\n",
        "    pass\n",
        "\n",
        "def predict(x, W, b):\n",
        "    \"\"\"Returns y_pred given the input and learned parameters.\"\"\"\n",
        "    # COMPLETE HERE (hint: use the sigmoid function above, and np.dot)\n",
        "    # Output should have shape (num_samples, 1)\n",
        "    pass\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    \"\"\"Returns the cross-entropy loss given the prediction and target.\"\"\"\n",
        "    # (hint: use np.log)\n",
        "    # Consider adding a small epsilon to the log if you run into errors.\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    # Ouput should have shape (num_samples, 1)\n",
        "    pass\n",
        "\n",
        "def dLossdW(y_pred, y_true, x):\n",
        "    \"\"\"Comptues the derivative of the loss with respect to W.\"\"\"\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    # Output should have shape (num_samples, 2)\n",
        "    pass\n",
        "\n",
        "def dLossdb(y_pred, y_true):\n",
        "    \"\"\"Comptues the derivative of the loss with respect to b.\"\"\"\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    # Output should have shape (num_samples, 1)\n",
        "    pass\n",
        "\n",
        "### DON'T MODIFY BELOW\n",
        "\n",
        "def gradient_descent_solver(x, y_true):\n",
        "    # Initialize weights\n",
        "    W = np.array([0.0, 0.0])[:, None]\n",
        "    b = np.array([0])\n",
        "    alpha = 1.0\n",
        "    num_steps = 1000\n",
        "\n",
        "    # Perform steps of gradient descent\n",
        "    y_pred = predict(x, W, b)\n",
        "    L_start = loss(y_pred, y_true).mean()\n",
        "    accuracy_start = ((y_pred > 0.5) == y_true).mean()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        y_pred = predict(x, W, b)\n",
        "        L = loss(y_pred, y_true).mean()\n",
        "        accuracy = ((y_pred > 0.5) == y_true).mean()\n",
        "\n",
        "        dW = dLossdW(y_pred, y_true, x)\n",
        "        db = dLossdb(y_pred, y_true)\n",
        "        W = W - alpha * dW.mean(axis=0)[:, None]\n",
        "        b = b - alpha * db.mean(axis=0)\n",
        "\n",
        "    print(\"Start loss: \", L_start)\n",
        "    print(\"Final loss: \", L)\n",
        "\n",
        "    print(\"Start accuracy: \", accuracy_start)\n",
        "    print(\"Final accuracy: \", accuracy)\n",
        "    return W, b\n",
        "\n",
        "def plot_results(x, y_true, W, b):\n",
        "    plt.figure()\n",
        "    plt.scatter(x[:, 0], x[:, 1], c=y_true)\n",
        "\n",
        "    x1 = np.linspace(-10, 10)\n",
        "    x2 = 0 * x1 - 0\n",
        "    plt.plot(x1, x2, c=\"b\", label=\"Starting boundary\")\n",
        "\n",
        "    x1 = np.linspace(-10, 10)\n",
        "    x2 = -W[0] / W[1] * x1 - b / W[1]\n",
        "    plt.plot(x1, x2, c=\"r\", label=\"Final boundary\")\n",
        "\n",
        "    plt.xlim(-10, 10)\n",
        "    plt.ylim(-10, 10)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Run training\n",
        "W1, b1 = gradient_descent_solver(X1, Y1)\n",
        "plot_results(X1, Y1, W1, b1)\n",
        "\n",
        "W2, b2 = gradient_descent_solver(X2, Y2)\n",
        "plot_results(X2, Y2, W2, b2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "saved-month",
      "metadata": {
        "id": "saved-month"
      },
      "source": [
        "**1.A.3 Question (2pt)**\n",
        "\n",
        "Comment on the plots above. How did the model perform on the first dataset? How about the second? Why is the model not appropriate for the second dataset?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "associate-grant",
      "metadata": {
        "id": "associate-grant"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "closed-architect",
      "metadata": {
        "id": "closed-architect"
      },
      "source": [
        "### 1.B Multi-layer Perceptron (MLP)\n",
        "\n",
        "Some datasets require more complex models to classify correctly. Let's see if we can improve the performance on the second dataset. This time we will implement a 2-layer MLP or FFN without an activation function. Mathematically, it performs multiplciation with some weights and adds the biases, and does that a second time with some different weight matrix and biases.\n",
        "\n",
        "$$y = (xW_1 + b_1)W_2 + b_2$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ordered-sullivan",
      "metadata": {
        "id": "ordered-sullivan"
      },
      "source": [
        "**1.B.1 Question (1pt)**\n",
        "\n",
        "Prove that the 2-layer model defined above isn't more powerful than a single layer model. (Hint: can you show the two layer model is still just the same as one layer using some simple algebraic manipulations?)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "verbal-juice",
      "metadata": {
        "id": "verbal-juice"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bizarre-saver",
      "metadata": {
        "id": "bizarre-saver"
      },
      "source": [
        "**1.B.2 Question (3pt)**\n",
        "\n",
        "In order to increase the modeling power of the model, we introduce a non-linear activation between the two layers. Here we choose the simple ReLU function:\n",
        "\n",
        "$$ ReLU(x) = max(0, x) $$\n",
        "\n",
        "There is a mathematical proof of the universal approximation theorem that shows how a two layer FFN with an activation function can model any function given enough training data, compute, and parameters (in practice, this does not work that well).\n",
        "\n",
        "We now have the following model:\n",
        "\n",
        "$$y = ReLU(xW_1 + b_1)W_2 + b_2$$\n",
        "\n",
        "The derivative is now more complex so we are going to use the PyTorch library which automates differentiation for us. All we need to do now is define our model and loss function. Fill out the code below, but now use Pytorch functions and not numpy.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "civil-original",
      "metadata": {
        "id": "civil-original",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def relu(x):\n",
        "    return torch.nn.functional.relu(x)\n",
        "\n",
        "def sigmoid(x):\n",
        "    \"\"\"Sigmoid activation of the input x.\"\"\"\n",
        "    # COMPLETE HERE (hint: use torch.exp)\n",
        "    # Well, there is a built in torch sigmoid function but\n",
        "    # please do it the hard way for learning purposes.\n",
        "    pass\n",
        "\n",
        "def predict(x, W1, b1, W2, b2):\n",
        "    \"\"\"Returns y_pred given the input and learned parameters.\"\"\"\n",
        "    # COMPLETE HERE (hint: use the sigmoid & relu functions above + torch.mm)\n",
        "    pass\n",
        "\n",
        "def loss(y_pred, y_true):\n",
        "    \"\"\"Returns the cross-entropy loss given the prediction and target.\"\"\"\n",
        "    # COMPLETE HERE (hint: use torch.log)\n",
        "    # Consider adding a small epsilon to the log if you run into errors.\n",
        "    # Return per sample (no averaging in the first dim)\n",
        "    pass\n",
        "\n",
        "### DON'T MODIFY BELOW\n",
        "\n",
        "def gradient_descent_solver(x, y_true):\n",
        "    # Initialize weights\n",
        "    random = np.random.RandomState(1)\n",
        "    W1 = random.randn(2, 100) * 0.01\n",
        "    W2 = random.randn(100, 1) * 0.01\n",
        "    W1 = torch.nn.Parameter(torch.tensor(W1).float())\n",
        "    b1 = torch.nn.Parameter(torch.zeros((100,)))\n",
        "    W2 = torch.nn.Parameter(torch.tensor(W2).float())\n",
        "    b2 = torch.nn.Parameter(torch.zeros((1,)))\n",
        "    alpha = 0.1\n",
        "    num_steps = 1000\n",
        "\n",
        "    # Perform steps of gradient descent\n",
        "    x = torch.tensor(x).float()\n",
        "    y_true = torch.tensor(y_true).float()\n",
        "    optimizer = torch.optim.SGD([W1, b1, W2, b2], alpha)\n",
        "\n",
        "    y_pred = predict(x, W1, b1, W2, b2)\n",
        "    L_start = loss(y_pred, y_true).mean()\n",
        "    accuracy_start = ((y_pred > 0.5) == y_true).float().mean()\n",
        "\n",
        "    for _ in range(num_steps):\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = predict(x, W1, b1, W2, b2)\n",
        "        L = loss(y_pred, y_true).mean()\n",
        "        L.backward()\n",
        "        optimizer.step()\n",
        "        accuracy = ((y_pred > 0.5) == y_true).float().mean()\n",
        "\n",
        "    print(\"Start loss: \", L_start.item())\n",
        "    print(\"Final loss: \", L.item())\n",
        "\n",
        "    print(\"Start accuracy: \", accuracy_start.item())\n",
        "    print(\"Final accuracy: \", accuracy.item())\n",
        "\n",
        "# Run training\n",
        "print(\"Dataset 1\")\n",
        "gradient_descent_solver(X1, Y1)\n",
        "\n",
        "print(\"\\nDataset 2\")\n",
        "gradient_descent_solver(X2, Y2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vital-commissioner",
      "metadata": {
        "id": "vital-commissioner"
      },
      "source": [
        "**1.B.3 Question (1pt)**\n",
        "\n",
        "How does the model perform now compared to the 1-layer model in the previous problem?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italian-output",
      "metadata": {
        "id": "italian-output"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "burning-irish",
      "metadata": {
        "id": "burning-irish"
      },
      "source": [
        "## 2. Modeling Small Molecules\n",
        "\n",
        "In this problem, you will experiment with various approaches to model small molecules with ML. The fundamental questions are: how can we represent molecules computationally? How can we predict the properties of molecules with these representations?\n",
        "\n",
        "For this problem, we consider the Tox21 dataset. The Tox21 dataset stores data in the form of **SMILES** strings, which is a standard format of describing chemical species. **Chem.MolFromSmiles** converts SMILEs strings into a representation that can be used by RDKit. Although multiple SMILES representations can depict a single small molecule, an algorithm exists to derive a unique canonical SMILES, ensuring that each molecule is represented by a singular, standardized string. In other words, there is a one to one mapping from small molecules to a cannonical SMILEs.\n",
        "\n",
        "First, visualize some of the molecules in the dataset using RDKit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wVMOhKfEhPv_",
      "metadata": {
        "id": "wVMOhKfEhPv_"
      },
      "outputs": [],
      "source": [
        "# Initialize graph transformation for chemical data processing\n",
        "# SMILESToBigraph: Converts SMILES strings to graph representations (bigraphs), which are undirected graphs.\n",
        "# node_featurizer: CanonicalAtomFeaturizer() transforms atoms in a molecule to feature vectors based on standard atomic properties.\n",
        "# edge_featurizer: CanonicalBondFeaturizer() transforms bonds between atoms into feature vectors, capturing bond type and properties.\n",
        "\n",
        "# Initialize the Tox21 dataset with the SMILESToBigraph transformation.\n",
        "# Tox21 dataset: Contains chemical structures as SMILES strings along with their biological activities for toxicity prediction.\n",
        "# The dataset will use the defined graph transformation (smiles_to_g) for preprocessing the SMILES strings into graph data suitable for graph-based machine learning.\n",
        "smiles_to_g = SMILESToBigraph(\n",
        "    node_featurizer=CanonicalAtomFeaturizer(),\n",
        "    edge_featurizer=CanonicalBondFeaturizer()\n",
        ")\n",
        "Tox21 = Tox21(smiles_to_g)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "secret-limitation",
      "metadata": {
        "id": "secret-limitation"
      },
      "outputs": [],
      "source": [
        "# Example 1\n",
        "print(Tox21[0][0])\n",
        "Chem.MolFromSmiles(Tox21[0][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "prerequisite-study",
      "metadata": {
        "id": "prerequisite-study"
      },
      "outputs": [],
      "source": [
        "# Example 2\n",
        "caffeine_smiles = \"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\"\n",
        "Chem.MolFromSmiles(caffeine_smiles)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p2xXngkJlRUj",
      "metadata": {
        "id": "p2xXngkJlRUj"
      },
      "source": [
        "After viewing the images of the the two examples, please take a look at this guide explaining various functional groups: https://www.masterorganicchemistry.com/2010/10/06/functional-groups-organic-chemistry/. In chemistry, a functional group is a particular arrangement of atoms and their bonds that have predictable chemical properties."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ob-7sSYcJ499",
      "metadata": {
        "id": "ob-7sSYcJ499"
      },
      "source": [
        "**2.A.1 Question**\n",
        "\n",
        "Can you identify a functional group in Example 1 and another functional group in Example 2? In very general terms, what are some properties of the functional groups you identified?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sMUNu2qPD4na",
      "metadata": {
        "id": "sMUNu2qPD4na"
      },
      "source": [
        "**ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "confused-berlin",
      "metadata": {
        "id": "confused-berlin"
      },
      "source": [
        "### 2.A MLP Over Fingerprints\n",
        "\n",
        "A Morgan fingerprint is a way of converting a chemical representation into a binary representation that respects chemical properties. As a result, a Morgan fingerprint can be used for simularity search, clustering, or as input to a deep learning model.\n",
        "\n",
        "We will train our first model using a simple MLP over Morgan Fingerprints. You can find more information about it here: https://www.rdkit.org/docs/GettingStartedInPython.html or by reviewing lecture slides."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "broken-currency",
      "metadata": {
        "id": "broken-currency"
      },
      "outputs": [],
      "source": [
        "fpgen = AllChem.GetMorganGenerator(radius=3)\n",
        "mol = Chem.MolFromSmiles(Tox21[0][0])\n",
        "ao = AllChem.AdditionalOutput()\n",
        "ao.CollectBitInfoMap()\n",
        "fp = fpgen.GetCountFingerprint(mol, additionalOutput=ao)\n",
        "arr = np.zeros((0,), dtype=np.int8)\n",
        "Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "print(\"Number of bits: \", len(arr))\n",
        "print(\"Total non zero entries:\", (arr > 0).sum())\n",
        "print(\"Maximum count:\", arr.max())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "powered-counter",
      "metadata": {
        "id": "powered-counter"
      },
      "source": [
        "The fingerprint is a count vector of length 2048, where each entry corresponds to a substrucuture and the number of times it appears in the molecule."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "turkish-basketball",
      "metadata": {
        "id": "turkish-basketball"
      },
      "source": [
        "**2.A.2 Question (1pt)**\n",
        "\n",
        "Find the most represented bit (maximum count in the array) and visualize it.  Where in the molecule does this pattern appear?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "impossible-priority",
      "metadata": {
        "id": "impossible-priority"
      },
      "outputs": [],
      "source": [
        "def most_represented_bit(arr):\n",
        "    # COMPLETE HERE\n",
        "    pass\n",
        "\n",
        "# Change num_item to any value < count for that bit\n",
        "num_item = 0\n",
        "bi = ao.GetBitInfoMap()\n",
        "idx = most_represented_bit(arr)\n",
        "Chem.Draw.DrawMorganBit(mol, idx, bi, whichExample=num_item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wrapped-emphasis",
      "metadata": {
        "id": "wrapped-emphasis"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ideal-contrast",
      "metadata": {
        "id": "ideal-contrast"
      },
      "source": [
        "**2.A.3 Question (2pt)**\n",
        "\n",
        "We process the Tox21 dataset so that we have the fingerprints for all molecules, and split the data randomly into training and testing. The Tox21 contains labels for various properties of these small molecules, and our goal is to predict these properties. We predict 12 labels for each molecule.\n",
        "\n",
        "Because the positive to negative ratio is imbalanced, we measure performance using the ROC-AUC (instead of accuracy) for each label individually. ROC-AUC measures the area under the curve that plots True Positive Rate against False Positive Rate. This metric is more comprehensive for assessing the quality of a model than simpler metrics.\n",
        "\n",
        "Implement a small MLP and train it. Try different values for the hidden dimension, fingerprint radius and dropout. Compare results and comment. Try at least 2 other combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "casual-today",
      "metadata": {
        "id": "casual-today"
      },
      "outputs": [],
      "source": [
        "# COMPLETE HERE (i.e. play with different values)\n",
        "RADIUS = 3\n",
        "HIDDEN_DIM = 128\n",
        "DROPOUT = 0\n",
        "NUM_EPOCHS = 10\n",
        "FP_SIZE = 2048\n",
        "\n",
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.w1 = torch.nn.Linear(in_dim, hidden_dim)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "        self.dropout = torch.nn.Dropout(DROPOUT)\n",
        "        self.w2 = torch.nn.Linear(hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        data = batch[\"data\"]\n",
        "        # COMPLETE HERE (hint: each pytorch layer can be called as a function)\n",
        "        pass\n",
        "\n",
        "# DON'T MODIFY BELOW\n",
        "def process_sample(sample):\n",
        "    smiles = sample[0]\n",
        "    labels = sample[2]\n",
        "    mask = sample[3]\n",
        "    fpgen = AllChem.GetMorganGenerator(radius=RADIUS, fpSize=FP_SIZE)\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    ao = AllChem.AdditionalOutput()\n",
        "    ao.CollectBitInfoMap()\n",
        "    fp = fpgen.GetCountFingerprint(mol, additionalOutput=ao)\n",
        "    arr = np.zeros((0,), dtype=np.int8)\n",
        "    Chem.DataStructs.ConvertToNumpyArray(fp, arr)\n",
        "    arr = torch.tensor(arr).float()\n",
        "    return {\"data\": arr, \"labels\": labels, \"mask\": mask}\n",
        "\n",
        "def create_dataset():\n",
        "    dataset = list(map(process_sample, Tox21))\n",
        "    train, test = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "    return train, test\n",
        "\n",
        "def create_model():\n",
        "    model = MLP(FP_SIZE, HIDDEN_DIM, 12)\n",
        "    return model\n",
        "\n",
        "def evaluate(model, dataloader):\n",
        "    out_pred, out_labels, out_mask = [], [], []\n",
        "    for batch in dataloader:\n",
        "        mask = batch[\"mask\"]\n",
        "        labels = batch[\"labels\"]\n",
        "        y_pred = model(batch).sigmoid()\n",
        "        out_pred.append(y_pred)\n",
        "        out_labels.append(labels)\n",
        "        out_mask.append(mask)\n",
        "\n",
        "    out_pred = torch.cat(out_pred).detach().numpy()\n",
        "    out_labels = torch.cat(out_labels).detach().numpy()\n",
        "    out_mask = torch.cat(out_mask).bool().detach().numpy()\n",
        "\n",
        "    aucs = []\n",
        "    for i in range(12):\n",
        "        preds = out_pred[:, i]\n",
        "        labels = out_labels[:, i]\n",
        "        mask = out_mask[:, i]\n",
        "        preds = preds[mask]\n",
        "        labels = labels[mask]\n",
        "        aucs.append(roc_auc_score(labels, preds))\n",
        "\n",
        "    return np.mean(aucs)\n",
        "\n",
        "def train(model, train_dataloader, test_dataloader, num_epochs):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    train_loss = []\n",
        "    train_aucs = []\n",
        "    test_aucs = []\n",
        "    for _ in tqdm(range(num_epochs), total=num_epochs):\n",
        "        avg_loss = 0\n",
        "        model.train()\n",
        "        for batch in train_dataloader:\n",
        "            optimizer.zero_grad()\n",
        "            mask = batch[\"mask\"]\n",
        "            labels = batch[\"labels\"]\n",
        "            y_pred = model(batch)\n",
        "            loss = torch.nn.functional.binary_cross_entropy_with_logits(y_pred, labels, reduction=\"none\")\n",
        "            loss = (loss * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1)\n",
        "            loss = loss.mean()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            avg_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            train_auc = evaluate(model, train_dataloader)\n",
        "            test_auc = evaluate(model, test_dataloader)\n",
        "\n",
        "        avg_loss /= len(train_dataloader)\n",
        "        train_loss.append(avg_loss)\n",
        "        train_aucs.append(train_auc)\n",
        "        test_aucs.append(test_auc)\n",
        "\n",
        "    return train_loss, train_aucs, test_aucs\n",
        "\n",
        "train_set, test_set = create_dataset()\n",
        "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True)\n",
        "test_dl = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False)\n",
        "model = create_model()\n",
        "train_loss, train_aucs, test_aucs = train(model, train_dl, test_dl, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
        "ax1.plot(train_loss)\n",
        "ax1.set_title(\"Training Loss\")\n",
        "ax2.plot(train_aucs)\n",
        "ax2.set_title(\"Training ROC-AUC\")\n",
        "ax3.plot(test_aucs)\n",
        "ax3.set_title(\"Test ROC-AUC\")\n",
        "\n",
        "print(\"Final Training ROC-AUC: \", train_aucs[-1])\n",
        "print(\"Best Training ROC-AUC: \", max(train_aucs))\n",
        "print(\"\\nFinal Testing ROC-AUC: \", test_aucs[-1])\n",
        "print(\"Best Testing ROC-AUC: \", max(test_aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "built-saint",
      "metadata": {
        "id": "built-saint"
      },
      "source": [
        "**2.A.4 Question (1pt)**\n",
        "\n",
        "How does the radius affect performance? How does the size of the hidden dimension affect performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "digital-jimmy",
      "metadata": {
        "id": "digital-jimmy"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "selective-despite",
      "metadata": {
        "id": "selective-despite"
      },
      "source": [
        "**2.A.5 Question (1pt)**\n",
        "\n",
        "Why does the test AUC initially increase with training but then starts to decline as training time increases? Try increasing the dropout. Does this phenomenon still occur?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "neither-award",
      "metadata": {
        "id": "neither-award"
      },
      "source": [
        "**ANSWER**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "happy-sarah",
      "metadata": {
        "id": "happy-sarah"
      },
      "source": [
        "### 2.B RNN over Smiles\n",
        "\n",
        "We now shift our attention to using reccurrent neural network (RNN) over smile strings. RNN's are used to encode sequences. At each step an RNN takes as input it's current state *h* and the input at that step *x*:\n",
        "\n",
        "$$ h_i = f(x_i, h_{i-1}) $$\n",
        "\n",
        "Consider the simplest example possible, where we apply a simple linear layer on the concatenated input and state vectors.\n",
        "\n",
        "$$ h_i = W [x_i, h_{i-1}] + b $$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "genetic-administrator",
      "metadata": {
        "id": "genetic-administrator"
      },
      "source": [
        "**2.B.1 Question (1pt)**\n",
        "\n",
        "This formulation, without activation functions, leads to exploding / vanishing gradients. Provide some intuition as to why that might be the case. (Hint: consider what happens to W as you start to unroll the computation across multiple steps, what happens to the eigenvalues of W? What does it imply when they are greater than 1, or lesser than 1)?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "religious-notice",
      "metadata": {
        "id": "religious-notice"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stopped-queensland",
      "metadata": {
        "id": "stopped-queensland"
      },
      "source": [
        "In order to address this challenge, and improve RNN's capacity to model long sequences, LSTM's were introduced. Without going in too much detail, LSTMs control the flow of information using learned input and output gates. This has been shown to be an effective way to model longer sequences. You can read more about them here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "constitutional-infection",
      "metadata": {
        "id": "constitutional-infection"
      },
      "source": [
        "**2.B.2 Question (3pt)**\n",
        "\n",
        "You will now implement an RNN / LSTM. Fill out the missing code below, here again play with some of the values for the hidden dimension and dropout. Comment on your observations. This make take a bit of time to train, so no need to try too many things."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "original-testing",
      "metadata": {
        "id": "original-testing"
      },
      "outputs": [],
      "source": [
        "# COMPLETE HERE (i.e. modify to a different value)\n",
        "HIDDEN_DIM = 100\n",
        "DROPOUT = 0\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = torch.nn.Embedding(in_dim, hidden_dim)\n",
        "        self.lstm = torch.nn.LSTM(\n",
        "            hidden_dim,\n",
        "            hidden_dim,\n",
        "            dropout=DROPOUT,\n",
        "            batch_first=True,\n",
        "            bidirectional=True\n",
        "        )\n",
        "        self.fc = torch.nn.Linear(2 * hidden_dim, out_dim)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        data = batch[\"data\"]\n",
        "        pad_mask = batch[\"pad_mask\"]\n",
        "        max_len = data.shape[1]\n",
        "\n",
        "        # COMPLETE HERE\n",
        "\n",
        "        # First we embed each input token into a vector\n",
        "        # Use the self.embedding layer\n",
        "        emb = ...\n",
        "\n",
        "        # Compute lengths from padding mask & use the above\n",
        "        lengths = ...\n",
        "\n",
        "        # In order to ignore padding we use\n",
        "        out = torch.nn.utils.rnn.pack_padded_sequence(emb, lengths=lengths, batch_first=True, enforce_sorted=False)\n",
        "\n",
        "        # Now we pass it to the LSTM (which outputs out, state). Ignore the state.\n",
        "        out = ...\n",
        "\n",
        "        # Now we unpack, we use\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(out, batch_first=True, total_length=max_len)[0]\n",
        "\n",
        "        # Compute the average vector for the sequence\n",
        "        # Beware of padding! Use the mask! Note that you will need to\n",
        "        # expand the mask to a 3D Tensor. You can do so with mask.unsqueeze(-1)\n",
        "        # you will also need to unsqueeze the lengths when dividing by them to take the average\n",
        "        out = ...\n",
        "\n",
        "        # Finally apply the fc layer\n",
        "        out = ...\n",
        "        return out\n",
        "\n",
        "\n",
        "# DON'T MODIFY THIS\n",
        "vocab = {\"~\": 0}\n",
        "def process_sample(sample, max_length):\n",
        "    smiles = sample[0]\n",
        "    labels = sample[2]\n",
        "    mask = sample[3]\n",
        "\n",
        "    tok_ids = []\n",
        "    for token in smiles:\n",
        "        if token not in vocab:\n",
        "            vocab[token] = len(vocab)\n",
        "            tok_id = len(vocab)\n",
        "        else:\n",
        "            tok_id = vocab[token]\n",
        "        tok_ids.append(tok_id)\n",
        "\n",
        "    arr = torch.tensor(tok_ids).long()\n",
        "    return {\"data\": arr, \"labels\": labels, \"mask\": mask}\n",
        "\n",
        "def create_dataset():\n",
        "    max_length = max(len(x[0]) for x in Tox21)\n",
        "    dataset = list(map(lambda x: process_sample(x, max_length), Tox21))\n",
        "    train, test = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "    return train, test\n",
        "\n",
        "def create_model():\n",
        "    return RNN(len(vocab) + 1, HIDDEN_DIM, 12)\n",
        "\n",
        "def collate_fn(data):\n",
        "\n",
        "    tok_ids = [d[\"data\"] for d in data]\n",
        "    pad_mask = [torch.ones_like(d[\"data\"]) for d in data]\n",
        "    labels = [d[\"labels\"] for d in data]\n",
        "    mask = [d[\"mask\"] for d in data]\n",
        "\n",
        "    tok_ids = torch.nn.utils.rnn.pad_sequence(tok_ids, batch_first=True)\n",
        "    pad_mask = torch.nn.utils.rnn.pad_sequence(pad_mask, batch_first=True)\n",
        "    labels = torch.stack(labels)\n",
        "    mask = torch.stack(mask)\n",
        "\n",
        "    return {\"data\": tok_ids, \"labels\": labels, \"mask\": mask, \"pad_mask\": pad_mask}\n",
        "\n",
        "\n",
        "train_set, test_set = create_dataset()\n",
        "model = create_model()\n",
        "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_dl = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "train_loss, train_aucs, test_aucs = train(model, train_dl, test_dl, num_epochs=NUM_EPOCHS)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
        "ax1.plot(train_loss)\n",
        "ax1.set_title(\"Training Loss\")\n",
        "ax2.plot(train_aucs)\n",
        "ax2.set_title(\"Training ROC-AUC\")\n",
        "ax3.plot(test_aucs)\n",
        "ax3.set_title(\"Test ROC-AUC\")\n",
        "\n",
        "print(\"Final Training ROC-AUC: \", train_aucs[-1])\n",
        "print(\"Best Training ROC-AUC: \", max(train_aucs))\n",
        "print(\"\\nFinal Testing ROC-AUC: \", test_aucs[-1])\n",
        "print(\"Best Testing ROC-AUC: \", max(test_aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "opened-lloyd",
      "metadata": {
        "id": "opened-lloyd"
      },
      "source": [
        "**2.B.3 Question (1pt)**\n",
        "\n",
        "How does the performance compare to the MLP baseline? Can you further improve it by modifying the hyperparameters? Try at least 2 other combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fifth-silly",
      "metadata": {
        "id": "fifth-silly"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "norman-circulation",
      "metadata": {
        "id": "norman-circulation"
      },
      "source": [
        "### 2.C GNN over Molecular Representations\n",
        "\n",
        "Moving on, we will now use the graph representation and a simple message passing neural network (MPNN). The input to GNNs generally, including MPNN, is a graph of $n$ nodes with up to $n^2$ edges. Small molecules are represents with atoms as nodes and bonds as edges.\n",
        "\n",
        "In general, graphs (including small molecules) are represented such that:\n",
        "\n",
        "1. The nodes are represented as a matrix of $n \\times h$, where each node is a $1 \\times h$ vector. So a graph's nodes is a list of 1-D vectors, or equivalently a 2D matrix.\n",
        "\n",
        "2. The edges are encoded into an adjacency matrix $M$ of shape $n \\times n$. $M[i][j] = 1$ when there is an edge from $i$ to $j$.\n",
        "\n",
        "3. Edges themselves have a vector representation encoding information about the edge. For every entry in $M[i][j]=1$, we create a 1_D vector $e_{ij}$ representing useful features of the edge. Then, we make a matrix $E = [e_{ij},...]$ containing an $e_{ij}$ representation for every edge.\n",
        "\n",
        "With such inputs, the MPNN does the following computations in a single layer:\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "m_{ij} &= MLP([h_i, h_j, e_{ij}]) \\\\\n",
        "h_i^{neigh} &= \\sum_j m_{ij} \\\\\n",
        "h_i^{new} &= MLP(h_i, h_i^{neigh})\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "The first equation is saying that for every edge, $e_{ij}$, we calculate a message by taking that edge and the two nodes it connects, and feeding it into an MLP. The output is a message $m_{ij}$\n",
        "\n",
        "Once we have all the messages $m_{ij}$, we calculating a new hidden representation called $h_i^{neigh}$ which works by taking a node $i$ and summing all the messages if that message came from an edge that touched that node.\n",
        "\n",
        "Lastly, the new hidden representation is an MLP that takes in $h_i^{neigh}$ and the old $h_i$ and its output is $h_i^{new}$ which updates the hidden representation for each node.\n",
        "\n",
        "Since this is a single layer, this can be done iteratively using many layers. Also, in the simple MPNN, edge representations $e_{ij}$ are not iteratively updated in a forward pass. But, it is possible, for instance, to make an embedding layer to generate edge representations, such that they can be updated during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "limiting-default",
      "metadata": {
        "id": "limiting-default"
      },
      "source": [
        "**2.C.1 Question (1pt)**\n",
        "\n",
        "For a given molecule, how many message passing steps would you need for every atom to \"see\" every other atom?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "natural-porcelain",
      "metadata": {
        "id": "natural-porcelain"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coated-warehouse",
      "metadata": {
        "id": "coated-warehouse"
      },
      "source": [
        "**2.C.2 Question (3pt)**\n",
        "\n",
        "Again complete the missing code and try a couple of different hyperparameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "indian-contribution",
      "metadata": {
        "id": "indian-contribution"
      },
      "outputs": [],
      "source": [
        "# COMPLETE HERE (i.e. modify to a different value)\n",
        "HIDDEN_DIM = 64\n",
        "NUM_STEPS = 4\n",
        "DROPOUT = 0\n",
        "EPOCHS = 20\n",
        "\n",
        "\n",
        "class GNNLayer(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, dim, dropout):\n",
        "        super().__init__()\n",
        "        self.message_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(dim * 3, dim),\n",
        "            torch.nn.SiLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(dim, dim)\n",
        "        )\n",
        "        self.node_mlp = torch.nn.Sequential(\n",
        "            torch.nn.Linear(dim * 2, dim),\n",
        "            torch.nn.SiLU(),\n",
        "            torch.nn.Dropout(dropout),\n",
        "            torch.nn.Linear(dim, dim)\n",
        "        )\n",
        "    def message(self, edges):\n",
        "        node_src = edges.src['h']\n",
        "        node_dst = edges.dst['h']\n",
        "        edge = edges.data['e']\n",
        "\n",
        "        # COMPLETE HERE (hint: use torch.cat in dim=-1 to concatenate and apply the message_mlp)\n",
        "        msg = ...\n",
        "        return {'msg_h': msg}\n",
        "\n",
        "    def forward(self, graph, nodes, edges):\n",
        "        with graph.local_scope():\n",
        "            # node feature\n",
        "            graph.ndata['h'] = nodes\n",
        "\n",
        "            # edge feature\n",
        "            graph.edata['e'] = edges\n",
        "\n",
        "            # Compute messages\n",
        "            graph.apply_edges(self.message)\n",
        "            graph.update_all(dgl.function.copy_e('msg_h', 'm'), dgl.function.sum('m', 'h_neigh'))\n",
        "            h_neigh = graph.ndata['h_neigh']\n",
        "\n",
        "            # Compute node updates\n",
        "            # COMPLETE HERE (hint: use torch.cat in dim=-1 to concatenate and apply the node mlp)\n",
        "            h_new = ...\n",
        "            return h_new\n",
        "\n",
        "\n",
        "# DON'T MODIFY THIS\n",
        "class GNN(torch.nn.Module):\n",
        "    def __init__(self, dim, dropout):\n",
        "        super().__init__()\n",
        "        self.node_fc = torch.nn.Linear(74, dim)\n",
        "        self.edge_fc = torch.nn.Linear(12, dim)\n",
        "        self.gnn = GNNLayer(dim, dropout)\n",
        "        self.fc = torch.nn.Linear(dim, 12)\n",
        "\n",
        "    def forward(self, batch):\n",
        "        g = batch[\"graph\"]\n",
        "        nodes = self.node_fc(g.ndata[\"h\"])\n",
        "        edges = self.edge_fc(g.edata[\"e\"])\n",
        "\n",
        "        for i in range(NUM_STEPS):\n",
        "            # We add a residual connection with helps with stability\n",
        "            nodes_new = self.gnn(g, nodes, edges)\n",
        "            nodes = nodes + torch.nn.functional.relu(nodes_new)\n",
        "\n",
        "        g.ndata[\"h_out\"] = nodes\n",
        "        out = dgl.mean_nodes(g, \"h_out\")\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "def process_sample(sample):\n",
        "    return {\"graph\": sample[1], \"labels\": sample[2], \"mask\": sample[3]}\n",
        "\n",
        "def create_dataset():\n",
        "    dataset = list(map(process_sample, Tox21))\n",
        "    train, test = train_test_split(dataset, test_size=0.2, random_state=1)\n",
        "    return train, test\n",
        "\n",
        "def create_model():\n",
        "    return GNN(HIDDEN_DIM, DROPOUT)\n",
        "\n",
        "def collate_fn(data):\n",
        "    graph = dgl.batch([d[\"graph\"] for d in data])\n",
        "    labels = torch.stack([d[\"labels\"] for d in data])\n",
        "    mask = torch.stack([d[\"mask\"] for d in data])\n",
        "    return {\"graph\": graph, \"labels\": labels, \"mask\": mask}\n",
        "\n",
        "\n",
        "train_set, test_set = create_dataset()\n",
        "model = create_model()\n",
        "train_dl = torch.utils.data.DataLoader(train_set, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
        "test_dl = torch.utils.data.DataLoader(test_set, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
        "train_loss, train_aucs, test_aucs = train(model, train_dl, test_dl, num_epochs=EPOCHS)\n",
        "\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(3)\n",
        "ax1.plot(train_loss)\n",
        "ax1.set_title(\"Training Loss\")\n",
        "ax2.plot(train_aucs)\n",
        "ax2.set_title(\"Training ROC-AUC\")\n",
        "ax3.plot(test_aucs)\n",
        "ax3.set_title(\"Test ROC-AUC\")\n",
        "\n",
        "print(\"Final Training ROC-AUC: \", train_aucs[-1])\n",
        "print(\"Best Training ROC-AUC: \", max(train_aucs))\n",
        "print(\"\\nFinal Testing ROC-AUC: \", test_aucs[-1])\n",
        "print(\"Best Testing ROC-AUC: \", max(test_aucs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "second-planning",
      "metadata": {
        "id": "second-planning"
      },
      "source": [
        "**2.C.3 Question (1pt)**\n",
        "\n",
        "How does the performance compare to the MLP and RNN baselines? Can you improve it by playing with hyperparameters (you still get full credit regardless)? Try at least 2 other combinations of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "certified-nickname",
      "metadata": {
        "id": "certified-nickname"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wNsGIlfFu2Gr",
      "metadata": {
        "id": "wNsGIlfFu2Gr"
      },
      "source": [
        "## 3. Molecular Docking\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9MYtEi_Ou5Vc",
      "metadata": {
        "id": "9MYtEi_Ou5Vc"
      },
      "source": [
        "In lecture we studied the task of molecular docking: given a protein target's structure in 3D, we wish to geometrically model the 3D coordinates of a binding partner. In this problem, we first review the question of symmetry preserving models, which is particuarly important to 3D tasks such as molecular docking. Then, we will run a few docking predictions using DiffDock (https://arxiv.org/abs/2210.01776) and analyze them."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ou1rwzdvDtD",
      "metadata": {
        "id": "3ou1rwzdvDtD"
      },
      "source": [
        "### 3.A Invariance & Equivariance\n",
        "\n",
        "Because the docking task is inherently a 3D task (i.e we are predicting coordinates), it's important to think about the geometrical symmetries that we wish our model to handle gracefully. We call this family of models \"invariant\" or \"equivariant\" to a group transformation. This is particularly neat property that significantly reduces the space of functions that the model considers, helping optimization and generalization.\n",
        "\n",
        "#### 3.A.1 Question\n",
        "\n",
        "What does it mean for a model to be invariant to translations and rotations of the input? How about equivariant? Give a simple mathematical definition using `f` as the model function, `x` as the input, `R` as a rotation matrix, and `t` as a translation vector."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2qfJmgz0vNf2",
      "metadata": {
        "id": "2qfJmgz0vNf2"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4IRHZtLUBUn0",
      "metadata": {
        "id": "4IRHZtLUBUn0"
      },
      "source": [
        "#### 3.A.2 Question\n",
        "\n",
        "A convolutional neural network has a particular invariant property that makes it suited for certain data types. Explain."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SysdKnz39Eq4",
      "metadata": {
        "id": "SysdKnz39Eq4"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BLs6Oc8uvRgN",
      "metadata": {
        "id": "BLs6Oc8uvRgN"
      },
      "source": [
        "#### 3.A.3 Question\n",
        "\n",
        "There are many ways to achieve invariance / equivariance, and it is currently a very active area of research. Here, we will study the the Equivariant Graph Neural Network (EGNN)  model by Satoras et. al: (https://arxiv.org/pdf/2102.09844.pdf).\n",
        "\n",
        "Recall the GNN layer with node features `h` and edge features `e`. We compute messages for each edge, aggregate the messages and used the result to update the features. We modify the formulas a little bit:\n",
        "\n",
        "$$ \\begin{align} m_{ij} &= MLP(h_i, h_j, e_{ij}) \\\\\n",
        " m_i &= \\frac{1}{N} \\sum_j m_{ij} \\\\\n",
        " \\tilde{h_i} &= MLP(h_i, m_i) \\end{align}$$\n",
        "\n",
        "\n",
        "An EGNN layer is very similar, defined by the following set of equations, but takes in new inputs $x$ which represent the 3-D (or 2-D or 1-D) spatial coordinates of the input. Please note that small molecules are not flat, and have interesting geometries. For an extreme example, take a look at buckminsterfullerene.\n",
        "\n",
        "\n",
        "Given a set of input features $h$ and coordinates $x$:\n",
        "\n",
        "$$ \\begin{align} m_{ij} &= MLP(h_i, h_j, e_{ij}, ||x_i - x_j||^2) \\\\\n",
        " m_i &= \\frac{1}{N} \\sum_j m_{ij} \\\\\n",
        "h_{new} &= MLP(h_i, m_i) \\\\\n",
        "x_{new} &= x_i + \\frac{1}{N} \\sum_j MLP(m_{ij}) \\cdot (\\vec{x_i}- \\vec{x_j}) \\end{align}$$\n",
        "\n",
        "\n",
        "The only changes are the use of pairwise distances in the first equation and the coordinate update in the last equation.\n",
        "\n",
        "Prove that EGNN is indeed equivariant to translation and rotation by showing that applying a rotation `R` and translation `t` to the input `x_i` and `x_j` is equivalent to applying it to the output `x_new`. Please mathematically typeset the solution using LaTex markup.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OlxyLj5UvcGe",
      "metadata": {
        "id": "OlxyLj5UvcGe"
      },
      "source": [
        "**ANSWER**:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3iLrJhb9vfO0",
      "metadata": {
        "id": "3iLrJhb9vfO0"
      },
      "source": [
        "#### 3.A.4 Question\n",
        "\n",
        "We will now implement the EGNN and verify its equivariance. We are not training the model on any dataset, but we have a test function to make sure that the implementation is actually equivariant."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YNefQ6Jgvs-K",
      "metadata": {
        "id": "YNefQ6Jgvs-K"
      },
      "outputs": [],
      "source": [
        "# HELPERS\n",
        "class MLP(object):\n",
        "  \"\"\"A random two layer neural network.\"\"\"\n",
        "\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    self.w1 = np.random.randn(in_dim, out_dim)\n",
        "    self.b1 = np.random.randn(out_dim)\n",
        "    self.w2 = np.random.randn(out_dim, out_dim)\n",
        "    self.b2 = np.random.randn(out_dim)\n",
        "\n",
        "  def forward(self, h):\n",
        "    h_new = np.dot(h, self.w1) + self.b1\n",
        "    h_new = h_new * (h_new > 0)\n",
        "    h_new = np.dot(h_new, self.w2) + self.b2\n",
        "    return h_new\n",
        "\n",
        "  def __call__(self, h):\n",
        "    return self.forward(h)\n",
        "\n",
        "def apply_transform(R, t, x):\n",
        "  \"\"\"Apply a rotation and translation to a set of points.\"\"\"\n",
        "  return np.dot(x, R.T) + t\n",
        "\n",
        "\n",
        "# COMPLETE BELOW\n",
        "class EGNN(object):\n",
        "  def __init__(self, dim):\n",
        "    \"\"\"Random weight initialization.\"\"\"\n",
        "    self.message_mlp = MLP(dim * 3 + 1, dim)\n",
        "    self.feature_mlp = MLP(dim * 2, dim)\n",
        "    self.coords_mlp = MLP(dim, 1)\n",
        "\n",
        "  def forward(self, h, e, x):\n",
        "    \"\"\"Apply an EGNN layer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    h:\n",
        "      node features as a numpy array of shape (N, D)\n",
        "    e:\n",
        "      edge features as a numpy array of shape (N, N, D)\n",
        "    x:\n",
        "      set of points in 3D as a numpy array of shape (N, 3)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    h_new:\n",
        "      new node features as a numpy array of shape (N, D)\n",
        "    x_new:\n",
        "      new set of points in 3D as a numpy array of shape (N, 3)\n",
        "\n",
        "    \"\"\"\n",
        "    # COMPLETE HERE\n",
        "\n",
        "    # Expand h to get all pairs of h's, your final dimension should be (N, N, 2 * D)\n",
        "    # You can expand with dummy dimensions and then use np.repeat\n",
        "\n",
        "    # Get all x_i - x_j vectors (you can just broadcast, without repeat)\n",
        "\n",
        "    # Compute the squared norm of the x_ij vectors\n",
        "\n",
        "    # Concatenate h pairs, distances, and edge features\n",
        "\n",
        "    # Compute messages for all i, j pairs\n",
        "\n",
        "    # Aggregate messages per node (sum across j)\n",
        "\n",
        "    # Update features\n",
        "\n",
        "    # Update coordinates\n",
        "    pass\n",
        "\n",
        "\n",
        "# DO NOT MODIFY BELOW\n",
        "def test_equivariance():\n",
        "  # Set parameters\n",
        "  N = 64\n",
        "  D = 32\n",
        "  h = np.random.randn(N, D)\n",
        "  e = np.random.randn(N, N, D)\n",
        "  x = np.random.randn(N, 3)\n",
        "  egnn = EGNN(D)\n",
        "\n",
        "  # Sample a random roto-translation\n",
        "  R = Rotation.random().as_matrix()\n",
        "  t = np.random.randn(3)\n",
        "  x_rot = apply_transform(R, t, x)\n",
        "\n",
        "  # Run model on x\n",
        "  _, x_new = egnn.forward(h, e, x)\n",
        "  x_new_rot = apply_transform(R, t, x_new)\n",
        "\n",
        "  # Run the model on x_rot\n",
        "  _, x_rot_new = egnn.forward(h, e, x_rot)\n",
        "\n",
        "  # Compare the outputs\n",
        "  if np.linalg.norm(x_new_rot - x_rot_new, axis=-1).max() < 1e-4:\n",
        "    print(\"Equivariance test passed!\")\n",
        "  else:\n",
        "    print(\"Equivariance test failed!\")\n",
        "\n",
        "\n",
        "test_equivariance()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aeis-rIvGYzN",
      "metadata": {
        "id": "aeis-rIvGYzN"
      },
      "source": [
        "## 4. DiffDock\n",
        "\n",
        "We will use a state of the art deep learning tool, DiffDock, to produce candidate docking structures. We will analyze the results to get some insight into the inner workings of the model. DiffDock also uses a 3D equivariant model, though it is not the EGNN introduced above but a tensor field network. These are more complex, and we will not study them in this course. If you are interested, you may read more about them here:\n",
        "https://arxiv.org/abs/1802.08219. For this problem, we will instead focus on a different charachteristic of DiffDock: the \"diffusion\" process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3vIk-iL0tf_f",
      "metadata": {
        "id": "3vIk-iL0tf_f"
      },
      "outputs": [],
      "source": [
        "!wget -c -O diffdock.zip https://www.dropbox.com/scl/fi/e23wyl4bfolckjaaablhc/diffdock.zip?rlkey=80p9a46yivyc3jdi7jnq7bnx9&dl=0\n",
        "!unzip -o diffdock.zip -d diffdock"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfLFhA-rIZLr",
      "metadata": {
        "id": "dfLFhA-rIZLr"
      },
      "source": [
        "#### 4.A.1 Question\n",
        "\n",
        "DiffDock is a specific type of generative model, namely a diffusion-based model. These models are trained to iteratively reverse a noise distribution (ex: gaussian) to the data distribution. Due to their generative nature, they can be sampled from, and every sample requires a certain number of denoising steps. For more info, see this very good blog post: https://lilianweng.github.io/posts/2021-07-11-diffusion-models/\n",
        "\n",
        "Interestingly, duffision models's loss functions are mathematically very similar to those used for training VAEs, despite looking different on the surface level: https://proceedings.neurips.cc/paper_files/paper/2023/hash/ce79fbf9baef726645bc2337abb0ade2-Abstract-Conference.html\n",
        "\n",
        "You are provided with pre-computed structures from DiffDock for this problem. If you are interested in making your own predictions you may use this colab notebook: https://colab.research.google.com/github/hgbrian/biocolabs/blob/master/DiffDock.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ub8kunF9J0uI",
      "metadata": {
        "id": "Ub8kunF9J0uI"
      },
      "outputs": [],
      "source": [
        "# RUN THIS\n",
        "DIFFDOCK_DATA = \"/content/diffdock/diffdock/\"\n",
        "GT_PATH = \"6o5g_B_LMJ.sdf\"\n",
        "\n",
        "ground_truth = os.path.join(DIFFDOCK_DATA, GT_PATH)\n",
        "predictions = []\n",
        "for file_name in os.listdir(DIFFDOCK_DATA):\n",
        "  if \"rank\" not in file_name:\n",
        "    continue\n",
        "  else:\n",
        "    meta = file_name.split(\"_\")\n",
        "    group = meta[0]\n",
        "    rank = int(meta[2][-1])\n",
        "    confidence = float(meta[3].split(\"confidence\")[1][:-4])\n",
        "    predictions.append({\n",
        "        \"path\": os.path.join(DIFFDOCK_DATA, file_name),\n",
        "        \"group\": group,\n",
        "        \"rank\": rank,\n",
        "        \"confidence\": confidence\n",
        "    })\n",
        "\n",
        "print(\"Ground truth path:\\n\")\n",
        "print(ground_truth)\n",
        "\n",
        "print(\"\\nPredictions:\")\n",
        "print(pd.DataFrame(predictions))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kQnu56nrFan8",
      "metadata": {
        "id": "kQnu56nrFan8"
      },
      "source": [
        "The table consists of 15 entries, each representing a different prediction made by the DiffDock model. These predictions are categorized based on three different sets of denoising steps: 5, 10, and 20. For each set of denoising steps, the model generates 5 distinct docking predictions, leading to a total of 15 unique prediction entries."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LMthsEUfJ3Oj",
      "metadata": {
        "id": "LMthsEUfJ3Oj"
      },
      "source": [
        "For each of the 3 groups (i.e 5, 10 and 20), report the best RMSD across the 5 samples. How did performance change as a function of the number of denoising steps?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PKz9miMuJ6Nv",
      "metadata": {
        "id": "PKz9miMuJ6Nv"
      },
      "outputs": [],
      "source": [
        "# HELPERS\n",
        "def load_coordinates_sdf(path):\n",
        "    \"\"\"Load molecule from sdf.\"\"\"\n",
        "    suppl = Chem.SDMolSupplier(path)\n",
        "    mol = next(suppl)\n",
        "    return mol\n",
        "\n",
        "def compute_rmsd(mol1, mol2):\n",
        "    \"\"\"Compute the RMSD between mol1 and mol2.\"\"\"\n",
        "    return Chem.rdMolAlign.CalcRMS(mol1, mol2)\n",
        "\n",
        "# COMPLETE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "p8Q3hjLfJ8eC",
      "metadata": {
        "id": "p8Q3hjLfJ8eC"
      },
      "source": [
        "**ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TZOS85fHKBPs",
      "metadata": {
        "id": "TZOS85fHKBPs"
      },
      "source": [
        "#### 4.A.2 Question\n",
        "\n",
        "Perform the same analysis, this time comparing strategies for measuring performance across the 5 samples:\n",
        "\n",
        "- The mean RMSD (same as before)\n",
        "- The best RMSD of the 5 samples\n",
        "- The highest confidence ranked of the 5 samples\n",
        "\n",
        "Comment on your observations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Zjb-PVniKD8t",
      "metadata": {
        "id": "Zjb-PVniKD8t"
      },
      "source": [
        "**ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FSvMNZlkKGXR",
      "metadata": {
        "id": "FSvMNZlkKGXR"
      },
      "source": [
        "#### 4.A.3 Question\n",
        "\n",
        "Visualize the worst and the best prediction (according to the RMSD) using the Molstar visualizer (https://molstar.org/viewer/).\n",
        "\n",
        "You can just drag and drop the files directly on your browser, super easy. Make sure to visualize both the sdf file (molecule prediction) and the ground truth PDB (target protein + correct molecule pose). Take screenshots for tbe best and worst predictions and display them here."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nr7AiQS5KH_c",
      "metadata": {
        "id": "nr7AiQS5KH_c"
      },
      "outputs": [],
      "source": [
        "# Set the best file and download it by running this cell\n",
        "# Do not include the /content/diffdock/diffdock prefix\n",
        "best_prediction = \"\"\n",
        "worst_prediction = \"\"\n",
        "\n",
        "# Download\n",
        "from google.colab import files\n",
        "\n",
        "DIFFDOCK_DATA = \"/content/diffdock/diffdock/\"\n",
        "files.download(os.path.join(DIFFDOCK_DATA, best_prediction))\n",
        "files.download(os.path.join(DIFFDOCK_DATA, worst_prediction))\n",
        "files.download(os.path.join(DIFFDOCK_DATA, \"6O5G.pdb\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "JdV1llkdKKrC",
      "metadata": {
        "id": "JdV1llkdKKrC"
      },
      "source": [
        "Do you notice anything interesting about the \"bad\" prediction?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "V3-v1nKnKNAF",
      "metadata": {
        "id": "V3-v1nKnKNAF"
      },
      "source": [
        "**ANSWER:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Qt28NfgAKXM-",
      "metadata": {
        "id": "Qt28NfgAKXM-"
      },
      "source": [
        "## 5. Genetic Association Studies and Finding eQTLs\n",
        "\n",
        "Genome-Wide Association Studies (GWAS) utilize genetic information, often derived from whole genome sequencing, to identify single nucleotide polymorphisms (SNPs) associated with various phenotypes. One notable success of GWAS was the identification of genetic variants in the CFH gene that significantly increase the risk of developing age-related macular degeneration (AMD), a leading cause of blindness in older adults.\n",
        "\n",
        "Beyond GWAS, genetic association studies can also examine how certain SNPs influence variations in gene expression, as observed in RNA sequencing (RNA-seq) experiments. These SNPs may reside within genomic regulatory regions, such as transcription factor binding sites or promoters, affecting gene expression levels directly.\n",
        "\n",
        "However, it's important to approach these findings with a high degree of scientific skepticism. Not all SNPs linked to variations in RNA-seq data are necessarily functional; some associations may be spurious. Rigorous validation and replication are essential to confirm the role of these SNPs as eQTLs—genetic variants that explain variations in RNA expression.\n",
        "\n",
        "In the final problem set question, we will explore how variations in gene expression, measured through RNA-seq, can differentiate subpopulations within a broader population. We will identify eQTLs that account for some of this variation in gene expression."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AdDgnkPrWzWl",
      "metadata": {
        "id": "AdDgnkPrWzWl"
      },
      "source": [
        "Please download Expdata.txt and SNPdata.txt from https://www.dropbox.com/scl/fo/soyclczbf0p4v1n7x0tnf/ALOOX7QjrI-e1BKqSFr6MQQ?rlkey=kddmo48uctnyu3jgn8xx4016t&e=2&dl=0\n",
        "\n",
        "Upload it to this notebook and work on the files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "I36PBnXkKfZS",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I36PBnXkKfZS",
        "outputId": "df426017-f185-4bdf-9348-2f6a9980e19b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-10-26 05:58:17--  https://www.dropbox.com/scl/fi/tjch877x78pglm0cqfuso/eQTLs.zip?rlkey=6tw9gotl098o8kshg67myfkwo&st=bnldxdln&dl=0\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com/cd/0/inline/CdJst1GaMEQkfWueAetTb7_N_ZT91hCiUqzvmzXh9xCWvlKjll7mcEHd2zsYKFNJxbm4p2vsztcf1-9noSCQEjXIXnzm6GKXuvvL8Va7SUruUDLpa5EgsIiuxeRBhI0BzybckHXU9wIgcbnuskoue3zQ/file# [following]\n",
            "--2024-10-26 05:58:17--  https://uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com/cd/0/inline/CdJst1GaMEQkfWueAetTb7_N_ZT91hCiUqzvmzXh9xCWvlKjll7mcEHd2zsYKFNJxbm4p2vsztcf1-9noSCQEjXIXnzm6GKXuvvL8Va7SUruUDLpa5EgsIiuxeRBhI0BzybckHXU9wIgcbnuskoue3zQ/file\n",
            "Resolving uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com (uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com (uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/CdJKHwoYT5XjYC06fEVRTxNNhs7HGchwUGIMpV4dHX5llALDLlJ1c-Yy75waDdsD8xRcxlO4ivGSWr_9-CQWx98mS5dumrL7yXo72FYkAgjhr2tzFkvIqlVqaHMGAUo2WqcyQzu0kItmUeOrvxFcgb07daiA_j1Jm6D3eGi3DXNUmSGylIEk9nND4U9DW_-dxBm6V0ogoR0hOFm3IUikE4GX4YlyC2DToJvow7MltGDuXVMLnSrXTbAuK6khHjrHTZ0ODFFeYhjiZzuMHxen5o1_cekAknMmwZas3_PNWjw9EL1Xw5pU97eBeKrAtlx9B8FjKmpFJJslCvaZlKeD6V-2rpCnJi78NGJ8agMjGj9GqZ4m5Y3uAeuRsj1ZkMymKXA/file [following]\n",
            "--2024-10-26 05:58:18--  https://uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com/cd/0/inline2/CdJKHwoYT5XjYC06fEVRTxNNhs7HGchwUGIMpV4dHX5llALDLlJ1c-Yy75waDdsD8xRcxlO4ivGSWr_9-CQWx98mS5dumrL7yXo72FYkAgjhr2tzFkvIqlVqaHMGAUo2WqcyQzu0kItmUeOrvxFcgb07daiA_j1Jm6D3eGi3DXNUmSGylIEk9nND4U9DW_-dxBm6V0ogoR0hOFm3IUikE4GX4YlyC2DToJvow7MltGDuXVMLnSrXTbAuK6khHjrHTZ0ODFFeYhjiZzuMHxen5o1_cekAknMmwZas3_PNWjw9EL1Xw5pU97eBeKrAtlx9B8FjKmpFJJslCvaZlKeD6V-2rpCnJi78NGJ8agMjGj9GqZ4m5Y3uAeuRsj1ZkMymKXA/file\n",
            "Reusing existing connection to uc5d77f62559c31ed44fab3b40bf.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 91836225 (88M) [application/zip]\n",
            "Saving to: ‘eQTLs.zip’\n",
            "\n",
            "eQTLs.zip           100%[===================>]  87.58M  80.6MB/s    in 1.1s    \n",
            "\n",
            "2024-10-26 05:58:19 (80.6 MB/s) - ‘eQTLs.zip’ saved [91836225/91836225]\n",
            "\n",
            "Archive:  eQTLs.zip\n",
            "warning:  stripped absolute path spec from /\n",
            "mapname:  conversion of  failed\n",
            " extracting: eqtl_data/SnpData.txt   \n",
            " extracting: eqtl_data/ExpData.txt   \n"
          ]
        }
      ],
      "source": [
        "!wget -c \"https://www.dropbox.com/scl/fi/tjch877x78pglm0cqfuso/eQTLs.zip?rlkey=6tw9gotl098o8kshg67myfkwo&st=bnldxdln&dl=0\" -O eQTLs.zip\n",
        "!unzip -o eQTLs.zip -d eqtl_data\n",
        "os.unlink(\"eQTLs.zip\")\n",
        "eqtl_data = {}\n",
        "for fname in os.listdir(\"eqtl_data\"):\n",
        "    with open(os.path.join(\"eqtl_data\", fname), \"r\") as R:\n",
        "        eqtl_data[fname] = R.read().encode('utf-8')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eId13Ax-Kehc",
      "metadata": {
        "id": "eId13Ax-Kehc"
      },
      "source": [
        "You are free to use the following function to help you process the raw `ExpData.txt` and `SnpData.txt` files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R_8J8iQeP1UN",
      "metadata": {
        "id": "R_8J8iQeP1UN"
      },
      "outputs": [],
      "source": [
        "def process_file(inp, mode):\n",
        "  \"\"\"\n",
        "  Processes a simplified representation of genetic or gene expression data from *Data.txt files.\n",
        "\n",
        "  This function is designed for educational purposes or small datasets and does not handle\n",
        "  the complexity typically associated with real-world genomic data formats like VCF or BAM.\n",
        "  It assumes input data is structured in a basic text format with space-separated values, where\n",
        "  the first line contains headers (traits or gene identifiers) and subsequent lines contain\n",
        "  numerical data representing gene expression levels or SNP values for each sample.\n",
        "\n",
        "  Parameters:\n",
        "      inp (bytes): The input data in bytes format, typically read from a file.\n",
        "      mode (str): The processing mode - 'patient' returns data by samples, 'trait' returns data by gene/trait.\n",
        "\n",
        "  Returns:\n",
        "      list: If mode is 'patient', returns a list of lists with each sublist containing\n",
        "      floats representing gene expression levels for each patient.\n",
        "      dict: If mode is 'trait', returns a dictionary where keys are traits or gene\n",
        "      identifiers and values are lists of floats representing gene expression levels across samples.\n",
        "\n",
        "  Example:\n",
        "      Input example:\n",
        "          Patient Trait1 Trait2 Trait3\n",
        "          A 0.1 0.2 0.3\n",
        "          B 0.4 0.5 0.6\n",
        "          C 0.7 0.8 0.9\n",
        "\n",
        "      Usage:\n",
        "          process_file(inp, \"patient\")\n",
        "              Returns: [[0.1, 0.2, 0.3], [0.4, 0.5, 0.6], [0.7, 0.8, 0.9]]\n",
        "          process_file(inp, \"trait\")\n",
        "              Returns: {\"Trait1\": [0.1, 0.4, 0.7], \"Trait2\": [0.2, 0.5, 0.8], \"Trait3\": [0.3, 0.6, 0.9]}\n",
        "  \"\"\"\n",
        "  rows = inp.decode(\"utf-8\").split(\"\\n\")\n",
        "  if mode == \"patient\":\n",
        "      return [[float(val) for val in row.split()[1:]] for row in rows[1:] if row]\n",
        "  elif mode == \"trait\":\n",
        "      res = pd.DataFrame([row.split()[1:] for row in rows if row])\n",
        "      res.columns = res.iloc[0]\n",
        "      return {k: [float(val) for val in v.values()] for k, v in res[1:].to_dict().items()}\n",
        "\n",
        "#process_file(eqtl_data[\"SnpData.txt\"], \"patient\")\n",
        "#process_file(eqtl_data[\"SnpData.txt\"], \"trait\")\n",
        "#process_file(eqtl_data[\"ExpData.txt\"], \"patient\")\n",
        "#process_file(eqtl_data[\"ExpData.txt\"], \"trait\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eMwzDmJP9NQ",
      "metadata": {
        "id": "0eMwzDmJP9NQ"
      },
      "source": [
        "**5.A.1**\n",
        "\n",
        "The `ExpData.txt` file contains log-normalized RNA-seq expression data from our population of 1,000 samples, with 5,000 genes profiled for each sample. Do a principal components analysis on this dataset to find the clusters of samples that have similar patterns of gene expression. Plot the output of your analysis.\n",
        "In your plots, be sure the axes are labeled with the components you are displaying in each plot. Also make sure that at least one of your plots colours the points corresponding to the samples with the sub-population that you think they should belong to. (Hint: You can modify your $k$-means code from PSET 1 to find these sub-populations. However, it is better to use kmeans packages such as `sklearn.cluster import KMeans`)\n",
        "\n",
        "You may find the `matplotlib.pyplot` and `sklearn.decomposition.PCA` packages (already imported) to be useful."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fgRDOfGuP_f9",
      "metadata": {
        "id": "fgRDOfGuP_f9"
      },
      "outputs": [],
      "source": [
        "def analyze_expression_data(data):\n",
        "  \"\"\"\n",
        "  After running process_file(..., ...), perform PCA on the data and\n",
        "  plot the first two principal components. (Since the plot is 2D, we can only\n",
        "  plot two principal components). Apply clustering as well to see the\n",
        "  structure of the data. On the plot, please color clusters that you find.\n",
        "  \"\"\"\n",
        "  pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mD7d9nUn_RzN",
      "metadata": {
        "id": "mD7d9nUn_RzN"
      },
      "source": [
        "Describe the patterns that you observe. What is the structure inherent in this population?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kqUYUD70Zb54",
      "metadata": {
        "id": "kqUYUD70Zb54"
      },
      "source": [
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nkbihXrvQEdg",
      "metadata": {
        "id": "nkbihXrvQEdg"
      },
      "source": [
        "**5.A.2**\n",
        "\n",
        "The `SnpData.txt` file contains genotyping data for the same 1,000 samples across 500 SNPs. Each SNP's genotype has been called with reference to the same reference genotype; \"0\" thus represents the reference allele, \"2\" represents the non-reference allele, and \"1\" represents a different allele on each strand.\n",
        "\n",
        "You will find that some of the SNPs (more than 5, less than 100) are eQTLs, that is, they have an effect on the expression of one or more of the genes we collected expression data for. Using whatever model you see fit, search for these eQTLs using the genotyping data and the expression data. **You may not have the computational resources to test all combinations of SNPs and genes, so you should think about smart ways to choose subsets of each to find some eQTLs - you don't have to find all of them!**\n",
        "\n",
        "For three of the eQTLs you found, present the evidence you have for why you think it is an eQTL, and not just associated with the expression of a gene by chance alone. *Be sure to include plots in your analysis to support your hypothesis, and to thoroughly explain the method you used to find eQTLs.* You can assume that the association between genotype and expression is linear for eQTLs. *Don't forget that you should be correcting for the fact that you are performing multiple significance tests.*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7pjJsWj5_jYE",
      "metadata": {
        "id": "7pjJsWj5_jYE"
      },
      "source": [
        "It may be difficult to think of the way to do it from scratch. Here is a general approach: perform PCA on the SNPs data and expression data (please experiment with different numbers of principal components). See if the SNP principal components are correlated with the expression principal components. Once you find pairs of correalated principal components, you want to map these back to the actual genes and SNPs. Then, when you have a short list of top genes and their SNPs, perform linear regression to show how gene expression of those genes can be a function of certain SNPs. There are other approaches that will have the same results as this, but this is the easiest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "i13vMIYqQGLI",
      "metadata": {
        "id": "i13vMIYqQGLI"
      },
      "outputs": [],
      "source": [
        "def process_and_pca(data):\n",
        "    \"\"\"\n",
        "    Apply PCA to reduce dimensionality.\n",
        "    Returns the PCA-transformed data.\n",
        "\n",
        "    Ideally, the data come from the functions:\n",
        "    #data = process_file(eqtl_data[\"SnpData.txt\"], \"patient\")\n",
        "    # process_file(eqtl_data[\"SnpData.txt\"], \"trait\")\n",
        "    # process_file(eqtl_data[\"ExpData.txt\"], \"patient\")\n",
        "    # process_file(eqtl_data[\"ExpData.txt\"], \"trait\")\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def correlation_and_top_pairs(exp_pcs, snp_pcs, top_n):\n",
        "    \"\"\"\n",
        "    Calculate correlation between expression and SNP PCA components, and return the top_n correlated pairs.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "def perform_regression_and_plot(exp_data, snp_data, regression_pairs):\n",
        "    \"\"\"\n",
        "    Perform regression analysis on selected gene-SNP pairs and plot the most significant.\n",
        "    Adjusts p-values for multiple testing and plots the two most important principal components of SNP data.\n",
        "    \"\"\"\n",
        "    pass\n",
        "\n",
        "# Example Usage\n",
        "exp_pcs = pass\n",
        "snp_pcs = pass\n",
        "\n",
        "top_corr_pairs = pass\n",
        "top_pairs = pass\n",
        "\n",
        "# Perform regression analysis and plot\n",
        "regression_results = pass"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "iY6WPdihQHkg",
      "metadata": {
        "id": "iY6WPdihQHkg"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "eQTL 1:\n",
        "\n",
        "eQTL 2:\n",
        "\n",
        "eQTL 3:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t5FelIzfQKKJ",
      "metadata": {
        "id": "t5FelIzfQKKJ"
      },
      "source": [
        "**5.A.3**\n",
        "\n",
        "In the above analysis, we were forced to consider all pairs of SNPs and genes to identify eQTLs. What sources of data that have not been provided as part of this problem would have been useful in constraining the amount of such pairs you had to test? For at least two sources:\n",
        "\n",
        "i. Give a description of what the dataset would look like (i.e. what are the rows and columns of the data matrix? what kinds of values are stored in the matrix?).\n",
        "\n",
        "ii. Explain how you would use it to filter out pairs of SNPs and genes that are unlikely to be associated with one another."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NiH5O3_YQMAg",
      "metadata": {
        "id": "NiH5O3_YQMAg"
      },
      "source": [
        "**Answer**:\n",
        "\n",
        "Source 1:\n",
        "\n",
        "i.\n",
        "\n",
        "ii.\n",
        "\n",
        "Source 2:\n",
        "\n",
        "i.\n",
        "\n",
        "ii."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}